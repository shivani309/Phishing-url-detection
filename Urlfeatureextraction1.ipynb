{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Test with URL\\nurl1 = str(input(\"Enter a URL to analyze: \"))\\n\\nfeatures_df = analyze_url(url1)\\nfeatures_df'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Initialize tldextract for domain extraction\n",
    "extractor = tldextract.TLDExtract(cache_dir=False)\n",
    "\n",
    "class feature_extractor:\n",
    "        def analyze_url(url):\n",
    "            features = {}\n",
    "            \n",
    "            # Parse URL components\n",
    "            extracted = extractor(url)\n",
    "            protocol = urlparse(url).scheme  # Get the protocol (http or https)\n",
    "            print(protocol)\n",
    "            full_domain = protocol + ('.' + extracted.subdomain if extracted.subdomain else '') + '.' + extracted.domain\n",
    "            \n",
    "            # 1. Domain Length (excluding protocol)\n",
    "            features['DomainLength'] = len(('.' + extracted.subdomain if extracted.subdomain else '') + '.' + extracted.domain)\n",
    "            \n",
    "            # 2. Is Domain an IP Address?\n",
    "            features['IsDomainIP'] = 1 if re.match(r'^\\d{1,3}(\\.\\d{1,3}){3}$', extracted.domain) else 0\n",
    "            \n",
    "            # 3. URL Similarity Index (Cosine similarity with risky keywords)\n",
    "            features['URLSimilarityIndex'] = calculate_similarity_index(url)\n",
    "            \n",
    "            # 4. Char Continuation Rate (rate of repeated characters in URL)\n",
    "            features['CharContinuationRate'] = calculate_continuation_rate(url)\n",
    "            \n",
    "            # 5. TLD Legitimate Probability (this could be complex, so it's a placeholder)\n",
    "            features['TLDLegitimateProb'] = 0.9  # Placeholder example value\n",
    "            \n",
    "            # 6. URL Character Probability (basic probability check using character frequencies)\n",
    "            features['URLCharProb'] = calculate_char_probability(url)\n",
    "            \n",
    "            # 7. TLD Length (length of the TLD part)\n",
    "            features['TLDLength'] = len(extracted.suffix)\n",
    "            \n",
    "            # 8. Number of Subdomains\n",
    "            features['NoOfSubDomain'] = len(extracted.subdomain.split('.')) if extracted.subdomain else 0\n",
    "            \n",
    "            # 9. Obfuscation: Check if URL has obfuscation patterns (%20, _, -, etc.)\n",
    "            features['HasObfuscation'] = 1 if has_obfuscation(url) else 0\n",
    "            features['NoOfObfuscatedChar'] = len(re.findall(r'%|\\.|_|-|@|%20', url))\n",
    "            features['ObfuscationRatio'] = features['NoOfObfuscatedChar'] / len(url) if len(url) > 0 else 0\n",
    "            \n",
    "            # 10. Letters in URL (only letters)\n",
    "            features['NoOfLettersInURL'] = sum(c.isalpha() for c in url)\n",
    "            features['LetterRatioInURL'] = features['NoOfLettersInURL'] / len(url) if len(url) > 0 else 0\n",
    "            \n",
    "            # 11. Digits in URL (only digits)\n",
    "            features['NoOfDegitsInURL'] = sum(c.isdigit() for c in url)\n",
    "            features['DegitRatioInURL'] = features['NoOfDegitsInURL'] / len(url) if len(url) > 0 else 0\n",
    "            \n",
    "            # 12. Count specific symbols in URL\n",
    "            features['NoOfEqualsInURL'] = url.count('=')\n",
    "            features['NoOfQMarkInURL'] = url.count('?')\n",
    "            features['NoOfAmpersandInURL'] = url.count('&')\n",
    "            features['NoOfOtherSpecialCharsInURL'] = len(re.findall(r'[^a-zA-Z0-9:/?&=]', url))\n",
    "            features['SpacialCharRatioInURL'] = features['NoOfOtherSpecialCharsInURL'] / len(url) if len(url) > 0 else 0\n",
    "            \n",
    "            # 13. HTTPS check\n",
    "            features['IsHTTPS'] = 1 if url.startswith('https://') else 0\n",
    "            \n",
    "            # Webpage-specific features (scraping)\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                features['LineOfCode'] = len(soup.prettify().splitlines())\n",
    "                features['LargestLineLength'] = max(len(line) for line in soup.prettify().splitlines())\n",
    "                \n",
    "                title_tag = soup.find('title')\n",
    "                features['HasTitle'] = 1 if title_tag else 0\n",
    "                features['DomainTitleMatchScore'] = calculate_domain_title_match_score(title_tag.text if title_tag else '', extracted.domain)\n",
    "                url_path = urlparse(url).path\n",
    "                features['URLTitleMatchScore'] = calculate_url_title_match_score(title_tag.text if title_tag else '', url_path)\n",
    "                \n",
    "                features['HasFavicon'] = 1 if soup.find('link', rel='icon') else 0\n",
    "                features['Robots'] = 1 if requests.get(url + '/robots.txt').status_code == 200 else 0\n",
    "                features['IsResponsive'] = 1 if soup.find('meta', attrs={'name': 'viewport'}) else 0\n",
    "                features['NoOfURLRedirect'] = len(response.history)\n",
    "                features['NoOfSelfRedirect'] = len(soup.find_all('a', href=lambda x: x and x.startswith(url)))\n",
    "                features['HasDescription'] = 1 if soup.find('meta', attrs={'name': 'description'}) else 0\n",
    "                features['NoOfPopup'] = count_popups(soup)\n",
    "                features['NoOfiFrame'] = len(soup.find_all('iframe'))\n",
    "                features['HasExternalFormSubmit'] = 1 if soup.find('form', action=lambda x: x and not x.startswith('/')) else 0\n",
    "                features['HasSocialNet'] = 1 if soup.find('a', href=lambda x: 'facebook.com' in x or 'twitter.com' in x) else 0\n",
    "                features['HasSubmitButton'] = 1 if soup.find('input', type='submit') else 0\n",
    "                features['HasHiddenFields'] = 1 if soup.find('input', type='hidden') else 0\n",
    "                features['HasPasswordField'] = 1 if soup.find('input', type='password') else 0\n",
    "                features['Bank'] = 1 if 'bank' in url.lower() else 0\n",
    "                features['Pay'] = 1 if 'pay' in url.lower() else 0\n",
    "                features['Crypto'] = 1 if 'crypto' in url.lower() else 0\n",
    "                features['HasCopyrightInfo'] = 1 if soup.find(text=re.compile(r'copyright', re.I)) else 0\n",
    "                features['NoOfImage'] = len(soup.find_all('img'))\n",
    "                features['NoOfCSS'] = len(soup.find_all('link', rel='stylesheet'))\n",
    "                features['NoOfJS'] = len(soup.find_all('script'))\n",
    "                features['NoOfSelfRef'] = len(soup.find_all('a', href=lambda x: x and x.startswith(url)))\n",
    "                features['NoOfEmptyRef'] = len(soup.find_all('a', href=''))\n",
    "                features['NoOfExternalRef'] = len(soup.find_all('a', href=lambda x: x and not x.startswith(url) and not x.startswith('/')))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching {url}: {e}\")\n",
    "            \n",
    "            return pd.DataFrame([features])\n",
    "\n",
    "        # Helper functions\n",
    "        def calculate_continuation_rate(url):\n",
    "            continuation_count = sum(url[i] == url[i + 1] for i in range(len(url) - 1))\n",
    "            return continuation_count / (len(url) - 1) if len(url) > 1 else 0\n",
    "\n",
    "        def calculate_similarity_index(url, keywords=[\"bank\", \"login\", \"secure\", \"account\"]):\n",
    "            vectorizer = TfidfVectorizer().fit_transform([url] + keywords)\n",
    "            vectors = vectorizer.toarray()\n",
    "            similarity_matrix = cosine_similarity(vectors)\n",
    "            return max(similarity_matrix[0][1:])\n",
    "\n",
    "        def calculate_char_probability(url):\n",
    "            char_counts = {char: url.count(char) for char in set(url)}\n",
    "            total_chars = len(url)\n",
    "            probabilities = {char: count / total_chars for char, count in char_counts.items()}\n",
    "            return sum(probabilities.values()) / len(probabilities) if probabilities else 0\n",
    "\n",
    "        def has_obfuscation(url):\n",
    "            return bool(re.search(r'%[0-9A-F]{2}|_|-|@|%20', url))\n",
    "\n",
    "        def calculate_domain_title_match_score(title, domain):\n",
    "            title_words = set(title.lower().split())\n",
    "            domain_words = set(domain.lower().split('.'))\n",
    "            return len(title_words & domain_words) / len(title_words) if title_words else 0\n",
    "\n",
    "        def calculate_url_title_match_score(title, path):\n",
    "            path_words = set(path.lower().split('/'))\n",
    "            title_words = set(title.lower().split())\n",
    "            return len(path_words & title_words) / len(path_words) if path_words else 0\n",
    "\n",
    "        def count_popups(soup):\n",
    "            # Custom function to count possible popups\n",
    "            return len(soup.find_all('script', src=lambda x: x and 'popup' in x))\n",
    "\n",
    "\"\"\"# Test with URL\n",
    "url1 = str(input(\"Enter a URL to analyze: \"))\n",
    "\n",
    "features_df = analyze_url(url1)\n",
    "features_df\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['featurextractor.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump , load \n",
    "featureext = feature_extractor()\n",
    "dump(featureext,'featurextractor.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
