{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in c:\\ProgramData\\anaconda3\\Lib\\site-packages\\tldextract\\.suffix_cache/publicsuffix.org-tlds\\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=False` to silence this warning. [WinError 5] Access is denied: 'c:\\\\ProgramData\\\\anaconda3\\\\Lib\\\\site-packages\\\\tldextract\\\\.suffix_cache'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching https://www.southbankmosaics.com: argument of type 'NoneType' is not iterable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILENAME</th>\n",
       "      <th>URL</th>\n",
       "      <th>URLLength</th>\n",
       "      <th>Domain</th>\n",
       "      <th>DomainLength</th>\n",
       "      <th>IsDomainIP</th>\n",
       "      <th>TLD</th>\n",
       "      <th>URLSimilarityIndex</th>\n",
       "      <th>CharContinuationRate</th>\n",
       "      <th>TLDLegitimateProb</th>\n",
       "      <th>...</th>\n",
       "      <th>URLTitleMatchScore</th>\n",
       "      <th>HasFavicon</th>\n",
       "      <th>Robots</th>\n",
       "      <th>IsResponsive</th>\n",
       "      <th>NoOfURLRedirect</th>\n",
       "      <th>NoOfSelfRedirect</th>\n",
       "      <th>HasDescription</th>\n",
       "      <th>NoOfPopup</th>\n",
       "      <th>NoOfiFrame</th>\n",
       "      <th>HasExternalFormSubmit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>www.southbankmosaics.com</td>\n",
       "      <td>https://www.southbankmosaics.com</td>\n",
       "      <td>32</td>\n",
       "      <td>southbankmosaics</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>com</td>\n",
       "      <td>0</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   FILENAME                               URL  URLLength  \\\n",
       "0  www.southbankmosaics.com  https://www.southbankmosaics.com         32   \n",
       "\n",
       "             Domain  DomainLength  IsDomainIP  TLD  URLSimilarityIndex  \\\n",
       "0  southbankmosaics            24           0  com                   0   \n",
       "\n",
       "   CharContinuationRate  TLDLegitimateProb  ...  URLTitleMatchScore  \\\n",
       "0              0.129032                0.9  ...                   0   \n",
       "\n",
       "   HasFavicon  Robots  IsResponsive  NoOfURLRedirect  NoOfSelfRedirect  \\\n",
       "0           0       1             1                0                 0   \n",
       "\n",
       "   HasDescription  NoOfPopup  NoOfiFrame  HasExternalFormSubmit  \n",
       "0               0          0           0                      1  \n",
       "\n",
       "[1 rows x 41 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "import re\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "# Use a custom cache directory\n",
    "extractor = tldextract.TLDExtract(cache_dir='/path/to/writable/cache_dir')\n",
    "extractor = tldextract.TLDExtract(cache_dir=False)\n",
    "\n",
    "def analyze_url(url):\n",
    "    features = {}\n",
    "    \n",
    "   \n",
    "    \n",
    "  \n",
    "    # 3. URLLength\n",
    "    features['URLLength'] = len(url)\n",
    "    \n",
    "\n",
    "    \n",
    "    # 5. DomainLength\n",
    "    #features['DomainLength'] = len(features['Domain'])\n",
    "    protocol = urlparse(url).scheme  # Get the protocol (http or https)\n",
    "    full_domain = protocol + ('.' + extracted.subdomain if extracted.subdomain else '') + '.' + features['Domain']\n",
    "    features['DomainLength'] = sum(c.isalpha() for c in full_domain)\n",
    "\n",
    "    # 6. IsDomainIP\n",
    "    features['IsDomainIP'] = 1 if re.match(r'^\\d{1,3}(\\.\\d{1,3}){3}$', extracted.domain) else 0\n",
    "    \n",
    "    # 7. TLD\n",
    "    features['TLD'] = extracted.suffix\n",
    "    \n",
    "    # 8. URLSimilarityIndex (Placeholder for now)\n",
    "    features['URLSimilarityIndex'] = 0  # Implement actual logic for similarity score\n",
    "    \n",
    "    # 9. CharContinuationRate\n",
    "    features['CharContinuationRate'] = calculate_continuation_rate(url)\n",
    "    \n",
    "    # 10. TLDLegitimateProb (Placeholder for now)\n",
    "    features['TLDLegitimateProb'] = 0.9  # Example value, implement actual logic\n",
    "    \n",
    "    # 11. URLCharProb (Placeholder for now)\n",
    "    features['URLCharProb'] = 0.9  # Example value, implement actual logic\n",
    "    \n",
    "    # 12. TLDLength\n",
    "    features['TLDLength'] = len(features['TLD'])\n",
    "    \n",
    "    # 13. NoOfSubDomain\n",
    "    features['NoOfSubDomain'] = len(extracted.subdomain.split('.')) if extracted.subdomain else 0\n",
    "    \n",
    "    # 14. HasObfuscation and 15. NoOfObfuscatedChar\n",
    "    obfuscation_chars = re.findall(r'%|\\.|_|-|@|%20', url)\n",
    "    features['HasObfuscation'] = 1 if obfuscation_chars else 0\n",
    "    features['NoOfObfuscatedChar'] = len(''.join(obfuscation_chars))\n",
    "    \n",
    "    # 16. ObfuscationRatio\n",
    "    features['ObfuscationRatio'] = features['NoOfObfuscatedChar'] / features['URLLength'] if features['URLLength'] > 0 else 0\n",
    "    \n",
    "    # 17. NoOfLettersInURL\n",
    "    features['NoOfLettersInURL'] = sum(c.isalpha() for c in url)\n",
    "    \n",
    "    # 18. LetterRatioInURL\n",
    "    features['LetterRatioInURL'] = features['NoOfLettersInURL'] / features['URLLength'] if features['URLLength'] > 0 else 0\n",
    "    \n",
    "    # 19. NoOfDigitsInURL\n",
    "    features['NoOfDigitsInURL'] = sum(c.isdigit() for c in url)\n",
    "    \n",
    "    # 20. DigitRatioInURL\n",
    "    features['DigitRatioInURL'] = features['NoOfDigitsInURL'] / features['URLLength'] if features['URLLength'] > 0 else 0\n",
    "    \n",
    "    # 21. NoOfEqualsInURL\n",
    "    features['NoOfEqualsInURL'] = url.count('=')\n",
    "    \n",
    "    # 22. NoOfQMarkInURL\n",
    "    features['NoOfQMarkInURL'] = url.count('?')\n",
    "    \n",
    "    # 23. NoOfAmpersandInURL\n",
    "    features['NoOfAmpersandInURL'] = url.count('&')\n",
    "    \n",
    "    # 24. NoOfOtherSpecialCharsInURL\n",
    "    features['NoOfOtherSpecialCharsInURL'] = len(re.findall(r'[^a-zA-Z0-9:/?&=]', url))\n",
    "    \n",
    "    # 25. SpecialCharRatioInURL\n",
    "    features['SpecialCharRatioInURL'] = features['NoOfOtherSpecialCharsInURL'] / features['URLLength'] if features['URLLength'] > 0 else 0\n",
    "    \n",
    "    # 26. IsHTTPS\n",
    "    features['IsHTTPS'] = 1 if url.startswith('https://') else 0\n",
    "    \n",
    "    # 27. LineOfCode, 28. LargestLineLength, and other website features\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # 27. LineOfCode (Approximation)\n",
    "        features['LineOfCode'] = len(soup.prettify().splitlines())\n",
    "        \n",
    "        # 28. LargestLineLength\n",
    "        features['LargestLineLength'] = max(len(line) for line in soup.prettify().splitlines())\n",
    "        \n",
    "        # 29. HasTitle\n",
    "        title_tag = soup.find('title')\n",
    "        features['HasTitle'] = 1 if title_tag else 0\n",
    "        \n",
    "        # 30. Title\n",
    "        features['Title'] = title_tag.text if title_tag else ''\n",
    "        \n",
    "        # 31. DomainTitleMatchScore (Placeholder)\n",
    "        features['DomainTitleMatchScore'] = 0  # Implement actual logic\n",
    "        \n",
    "        # 32. URLTitleMatchScore (Placeholder)\n",
    "        features['URLTitleMatchScore'] = 0  # Implement actual logic\n",
    "        \n",
    "        # 33. HasFavicon\n",
    "        features['HasFavicon'] = 1 if soup.find('link', rel='icon') else 0\n",
    "        \n",
    "        # 34. Robots\n",
    "        features['Robots'] = 1 if requests.get(url + '/robots.txt').status_code == 200 else 0\n",
    "        \n",
    "        # 35. IsResponsive (Simple check)\n",
    "        features['IsResponsive'] = 1 if soup.find('meta', attrs={'name': 'viewport'}) else 0\n",
    "        \n",
    "        # 36. NoOfURLRedirect\n",
    "        features['NoOfURLRedirect'] = len(response.history)\n",
    "        \n",
    "        # 37. NoOfSelfRedirect (Not easy to implement without actual logging)\n",
    "        features['NoOfSelfRedirect'] = 0  # Implement actual logic\n",
    "        \n",
    "        # 38. HasDescription\n",
    "        features['HasDescription'] = 1 if soup.find('meta', attrs={'name': 'description'}) else 0\n",
    "        \n",
    "        # 39. NoOfPopup and 40. NoOfiFrame (Approximation)\n",
    "        features['NoOfPopup'] = 0  # Implement actual logic\n",
    "        features['NoOfiFrame'] = len(soup.find_all('iframe'))\n",
    "        \n",
    "        # 41. HasExternalFormSubmit\n",
    "        features['HasExternalFormSubmit'] = 1 if soup.find('form', action=lambda x: x and not x.startswith('/')) else 0\n",
    "        \n",
    "        # 42. HasSocialNet\n",
    "        features['HasSocialNet'] = 1 if soup.find('a', href=lambda x: 'facebook.com' in x or 'twitter.com' in x) else 0\n",
    "        \n",
    "        # 43. HasSubmitButton\n",
    "        features['HasSubmitButton'] = 1 if soup.find('input', type='submit') else 0\n",
    "        \n",
    "        # 44. HasHiddenFields\n",
    "        features['HasHiddenFields'] = 1 if soup.find('input', type='hidden') else 0\n",
    "        \n",
    "        # 45. HasPasswordField\n",
    "        features['HasPasswordField'] = 1 if soup.find('input', type='password') else 0\n",
    "        \n",
    "        # 46. Bank, 47. Pay, 48. Crypto (Placeholder)\n",
    "        features['Bank'] = 1 if 'bank' in url.lower() else 0\n",
    "        features['Pay'] = 1 if 'pay' in url.lower() else 0\n",
    "        features['Crypto'] = 1 if 'crypto' in url.lower() else 0\n",
    "        \n",
    "        # 49. HasCopyrightInfo\n",
    "        features['HasCopyrightInfo'] = 1 if soup.find(text=re.compile(r'copyright', re.I)) else 0\n",
    "        \n",
    "        # 50. NoOfImage\n",
    "        features['NoOfImage'] = len(soup.find_all('img'))\n",
    "        \n",
    "        # 51. NoOfCSS\n",
    "        features['NoOfCSS'] = len(soup.find_all('link', rel='stylesheet'))\n",
    "        \n",
    "        # 52. NoOfJS\n",
    "        features['NoOfJS'] = len(soup.find_all('script'))\n",
    "        \n",
    "        # 53. NoOfSelfRef (Approximation)\n",
    "        features['NoOfSelfRef'] = len(soup.find_all('a', href=lambda x: x and x.startswith(url)))\n",
    "        \n",
    "        # 54. NoOfEmptyRef\n",
    "        features['NoOfEmptyRef'] = len(soup.find_all('a', href=''))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "    \n",
    "    df = pd.DataFrame([features])  # Creating DataFrame from features dictionary\n",
    "    return df\n",
    "\n",
    "def calculate_continuation_rate(url):\n",
    "    \"\"\"\n",
    "    Calculate the rate of character continuation in the URL.\n",
    "    This is a simplified version and can be improved.\n",
    "    \"\"\"\n",
    "    continuation_count = sum(url[i] == url[i + 1] for i in range(len(url) - 1))\n",
    "    return continuation_count / (len(url) - 1) if len(url) > 1 else 0\n",
    "\n",
    "def clean_url(url):\n",
    "    # Remove any unwanted characters like tab\n",
    "    return url.strip()\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "url1 = input(\"Enter a URL to analyze: \")\n",
    "url = clean_url(url1)\n",
    "features_df = analyze_url(url)\n",
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['www.southbankmosaics.com', 'https://www.southbankmosaics.com',\n",
       "        32, 'southbankmosaics', 24, 0, 'com', 0, 0.12903225806451613,\n",
       "        0.9, 0.9, 3, 1, 1, 2, 0.0625, 27, 0.84375, 0, 0.0, 0, 0, 0, 2,\n",
       "        0.0625, 1, 1282, 9467, 1,\n",
       "        'ข่าวสด ข่าววันนี้ ข่าวกีฬา ข่าวบันเทิง อัพเดทสดใหม่ทุกวัน – ข่าวสด ข่าวกีฬา ข่าวบันเทิง ข่าววันนี้ อัปเดตข่าวสารรวดเร็วทันใจ พร้อมรับชมสาระน่ารู้ต่างๆ ได้ฟรีตลอด 24ชั่วโมง',\n",
       "        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1]], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cl501_27\\AppData\\Roaming\\Python\\Python311\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching https://www.southbankmosaics.com: argument of type 'NoneType' is not iterable\n",
      "             Domain  TLD  URLLength  DomainLength  IsDomainIP  \\\n",
      "0  southbankmosaics  com         32            24           0   \n",
      "\n",
      "   URLSimilarityIndex  CharContinuationRate  TLDLegitimateProb  URLCharProb  \\\n",
      "0            0.511357              0.129032               0.95      0.84375   \n",
      "\n",
      "   TLDLength  ...  URLTitleMatchScore  HasFavicon  Robots  IsResponsive  \\\n",
      "0          3  ...                   0           0       1             1   \n",
      "\n",
      "   NoOfURLRedirect  NoOfSelfRedirect  HasDescription  NoOfPopup  NoOfiFrame  \\\n",
      "0                0                 0               0          0           0   \n",
      "\n",
      "   HasExternalFormSubmit  \n",
      "0                      1  \n",
      "\n",
      "[1 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "import re\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Use a custom cache directory for TLD extraction\n",
    "extractor = tldextract.TLDExtract(cache_dir='/path/to/writable/cache_dir')\n",
    "extractor = tldextract.TLDExtract(cache_dir=False)\n",
    "\n",
    "# Example legitimate TLDs with associated probabilities\n",
    "tld_legitimacy = {\n",
    "    'com': 0.95, 'org': 0.9, 'net': 0.85, 'edu': 0.9, 'gov': 0.98,\n",
    "    'xyz': 0.5, 'info': 0.7, 'biz': 0.6  # Extend this dictionary as needed\n",
    "}\n",
    "\n",
    "def analyze_url(url):\n",
    "    features = {}\n",
    "\n",
    "    # Extract domain information\n",
    "    extracted = extractor(url)\n",
    "    features['Domain'] = extracted.domain\n",
    "    features['TLD'] = extracted.suffix\n",
    "\n",
    "    # URL Length\n",
    "    features['URLLength'] = len(url)\n",
    "\n",
    "    # Domain Length (sum of alphabetic characters in domain)\n",
    "    protocol = urlparse(url).scheme\n",
    "    full_domain = protocol + ('.' + extracted.subdomain if extracted.subdomain else '') + '.' + features['Domain']\n",
    "    features['DomainLength'] = sum(c.isalpha() for c in full_domain)\n",
    "\n",
    "    # Check if Domain is an IP address\n",
    "    features['IsDomainIP'] = 1 if re.match(r'^\\d{1,3}(\\.\\d{1,3}){3}$', extracted.domain) else 0\n",
    "\n",
    "    # URL Similarity Index using TF-IDF and cosine similarity\n",
    "    known_legit_urls = [\"https://www.google.com\", \"https://www.facebook.com\"]\n",
    "    url_similarity_scores = []\n",
    "    vectorizer = TfidfVectorizer().fit_transform([url] + known_legit_urls)\n",
    "    similarity_matrix = cosine_similarity(vectorizer[0:1], vectorizer)\n",
    "    features['URLSimilarityIndex'] = np.mean(similarity_matrix[0][1:])\n",
    "\n",
    "    # Char Continuation Rate\n",
    "    features['CharContinuationRate'] = calculate_continuation_rate(url)\n",
    "\n",
    "    # TLD Legitimacy Probability\n",
    "    features['TLDLegitimateProb'] = tld_legitimacy.get(features['TLD'], 0.5)\n",
    "\n",
    "    # URL Character Probability (based on ratio of alphanumeric to special characters)\n",
    "    special_characters = sum(1 for char in url if not char.isalnum())\n",
    "    features['URLCharProb'] = 1 - (special_characters / len(url)) if len(url) > 0 else 0\n",
    "\n",
    "    # TLD Length\n",
    "    features['TLDLength'] = len(features['TLD'])\n",
    "\n",
    "    # Number of Subdomains\n",
    "    features['NoOfSubDomain'] = len(extracted.subdomain.split('.')) if extracted.subdomain else 0\n",
    "\n",
    "    # Obfuscation Checks\n",
    "    obfuscation_chars = re.findall(r'%|\\.|_|-|@|%20', url)\n",
    "    features['HasObfuscation'] = 1 if obfuscation_chars else 0\n",
    "    features['NoOfObfuscatedChar'] = len(''.join(obfuscation_chars))\n",
    "    features['ObfuscationRatio'] = features['NoOfObfuscatedChar'] / features['URLLength'] if features['URLLength'] > 0 else 0\n",
    "\n",
    "    # Additional URL character ratios\n",
    "    features['NoOfLettersInURL'] = sum(c.isalpha() for c in url)\n",
    "    features['LetterRatioInURL'] = features['NoOfLettersInURL'] / features['URLLength'] if features['URLLength'] > 0 else 0\n",
    "    features['NoOfDigitsInURL'] = sum(c.isdigit() for c in url)\n",
    "    features['DigitRatioInURL'] = features['NoOfDigitsInURL'] / features['URLLength'] if features['URLLength'] > 0 else 0\n",
    "    features['NoOfEqualsInURL'] = url.count('=')\n",
    "    features['NoOfQMarkInURL'] = url.count('?')\n",
    "    features['NoOfAmpersandInURL'] = url.count('&')\n",
    "    features['NoOfOtherSpecialCharsInURL'] = len(re.findall(r'[^a-zA-Z0-9:/?&=]', url))\n",
    "    features['SpecialCharRatioInURL'] = features['NoOfOtherSpecialCharsInURL'] / features['URLLength'] if features['URLLength'] > 0 else 0\n",
    "    features['IsHTTPS'] = 1 if url.startswith('https://') else 0\n",
    "\n",
    "    # Line of Code, Largest Line Length, and other features based on website content\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Approximate number of lines of code\n",
    "        features['LineOfCode'] = len(soup.prettify().splitlines())\n",
    "        \n",
    "        # Largest Line Length\n",
    "        features['LargestLineLength'] = max(len(line) for line in soup.prettify().splitlines())\n",
    "        \n",
    "        # Title-based features\n",
    "        title_tag = soup.find('title')\n",
    "        features['HasTitle'] = 1 if title_tag else 0\n",
    "        features['Title'] = title_tag.text if title_tag else ''\n",
    "        \n",
    "        # Domain and URL Title Match Scores\n",
    "        features['DomainTitleMatchScore'] = fuzz.ratio(features['Domain'], features['Title']) if features['Title'] else 0\n",
    "        features['URLTitleMatchScore'] = fuzz.ratio(url, features['Title']) if features['Title'] else 0\n",
    "\n",
    "        # Additional features based on webpage structure\n",
    "        features['HasFavicon'] = 1 if soup.find('link', rel='icon') else 0\n",
    "        features['Robots'] = 1 if requests.get(url + '/robots.txt').status_code == 200 else 0\n",
    "        features['IsResponsive'] = 1 if soup.find('meta', attrs={'name': 'viewport'}) else 0\n",
    "        features['NoOfURLRedirect'] = len(response.history)\n",
    "        features['NoOfSelfRedirect'] = 0  # Self-redirect logic would require advanced logging\n",
    "        features['HasDescription'] = 1 if soup.find('meta', attrs={'name': 'description'}) else 0\n",
    "        features['NoOfPopup'] = 0  # Popup detection would require browser-based interaction\n",
    "        features['NoOfiFrame'] = len(soup.find_all('iframe'))\n",
    "        features['HasExternalFormSubmit'] = 1 if soup.find('form', action=lambda x: x and not x.startswith('/')) else 0\n",
    "        features['HasSocialNet'] = 1 if soup.find('a', href=lambda x: 'facebook.com' in x or 'twitter.com' in x) else 0\n",
    "        features['HasSubmitButton'] = 1 if soup.find('input', type='submit') else 0\n",
    "        features['HasHiddenFields'] = 1 if soup.find('input', type='hidden') else 0\n",
    "        features['HasPasswordField'] = 1 if soup.find('input', type='password') else 0\n",
    "        features['Bank'] = 1 if 'bank' in url.lower() else 0\n",
    "        features['Pay'] = 1 if 'pay' in url.lower() else 0\n",
    "        features['Crypto'] = 1 if 'crypto' in url.lower() else 0\n",
    "        features['HasCopyrightInfo'] = 1 if soup.find(text=re.compile(r'copyright', re.I)) else 0\n",
    "        features['NoOfImage'] = len(soup.find_all('img'))\n",
    "        features['NoOfCSS'] = len(soup.find_all('link', rel='stylesheet'))\n",
    "        features['NoOfJS'] = len(soup.find_all('script'))\n",
    "        features['NoOfSelfRef'] = len(soup.find_all('a', href=lambda x: x and x.startswith(url)))\n",
    "        features['NoOfEmptyRef'] = len(soup.find_all('a', href=''))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "    \n",
    "    # Convert features to DataFrame\n",
    "    df = pd.DataFrame([features])\n",
    "    return df\n",
    "\n",
    "def calculate_continuation_rate(url):\n",
    "    # Rate of character continuation\n",
    "    continuation_count = sum(url[i] == url[i + 1] for i in range(len(url) - 1))\n",
    "    return continuation_count / (len(url) - 1) if len(url) > 1 else 0\n",
    "\n",
    "# Clean URL input\n",
    "def clean_url(url):\n",
    "    return url.strip()\n",
    "\n",
    "# Example usage\n",
    "url1 = input(\"Enter a URL to analyze: \")\n",
    "url = clean_url(url1)\n",
    "features_df = analyze_url(url)\n",
    "print(features_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: fuzzywuzzy\n",
      "Successfully installed fuzzywuzzy-0.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Domain  URLLength  DomainLength  IsDomainIP TLD  URLSimilarityIndex  \\\n",
      "0  uni-mainz         24            16           0  de                 0.0   \n",
      "\n",
      "   CharContinuationRate  TLDLegitimateProb  URLCharProb  TLDLength  ...  \\\n",
      "0              0.173913                0.9          0.9          2  ...   \n",
      "\n",
      "   HasPasswordField  Bank  Pay  Crypto  HasCopyrightInfo  NoOfImage  NoOfCSS  \\\n",
      "0                 0     0    0       0                 0          2       18   \n",
      "\n",
      "   NoOfJS  NoOfSelfRef  NoOfEmptyRef  \n",
      "0      76            0             0  \n",
      "\n",
      "[1 rows x 51 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cl501_27\\AppData\\Local\\Temp\\ipykernel_11440\\207642453.py:113: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  features['HasCopyrightInfo'] = 1 if soup.find(text=re.compile(r'copyright', re.I)) else 0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Use a custom cache directory for tldextract if needed\n",
    "extractor = tldextract.TLDExtract(cache_dir=False)\n",
    "\n",
    "def analyze_url(url):\n",
    "    features = {}\n",
    "    \n",
    "    # Parse URL components\n",
    "    extracted = extractor(url)\n",
    "    features['Domain'] = extracted.domain\n",
    "    \n",
    "    # URL Length\n",
    "    features['URLLength'] = len(url)\n",
    "    \n",
    "    # Domain Length\n",
    "    protocol = urlparse(url).scheme  # Get the protocol (http or https)\n",
    "    full_domain = protocol + ('.' + extracted.subdomain if extracted.subdomain else '') + '.' + features['Domain']\n",
    "    features['DomainLength'] = sum(c.isalpha() for c in full_domain)\n",
    "\n",
    "    # IsDomainIP\n",
    "    features['IsDomainIP'] = 1 if re.match(r'^\\d{1,3}(\\.\\d{1,3}){3}$', extracted.domain) else 0\n",
    "    \n",
    "    # TLD\n",
    "    features['TLD'] = extracted.suffix\n",
    "    \n",
    "    # URL Similarity Index\n",
    "    features['URLSimilarityIndex'] = calculate_similarity_index(url)\n",
    "    \n",
    "    # Char Continuation Rate\n",
    "    features['CharContinuationRate'] = calculate_continuation_rate(url)\n",
    "    \n",
    "    # TLD Legitimate Probability (Placeholder)\n",
    "    features['TLDLegitimateProb'] = 0.9  # Example value\n",
    "    \n",
    "    # URL Character Probability (Placeholder)\n",
    "    features['URLCharProb'] = 0.9  # Example value\n",
    "    \n",
    "    # TLD Length\n",
    "    features['TLDLength'] = len(features['TLD'])\n",
    "    \n",
    "    # Number of Subdomains\n",
    "    features['NoOfSubDomain'] = len(extracted.subdomain.split('.')) if extracted.subdomain else 0\n",
    "    \n",
    "    # Obfuscation Characteristics\n",
    "    obfuscation_chars = re.findall(r'%|\\.|_|-|@|%20', url)\n",
    "    features['HasObfuscation'] = 1 if obfuscation_chars else 0\n",
    "    features['NoOfObfuscatedChar'] = len(''.join(obfuscation_chars))\n",
    "    features['ObfuscationRatio'] = features['NoOfObfuscatedChar'] / features['URLLength'] if features['URLLength'] > 0 else 0\n",
    "    \n",
    "    # URL Composition\n",
    "    features['NoOfLettersInURL'] = sum(c.isalpha() for c in url)\n",
    "    features['LetterRatioInURL'] = features['NoOfLettersInURL'] / features['URLLength'] if features['URLLength'] > 0 else 0\n",
    "    features['NoOfDigitsInURL'] = sum(c.isdigit() for c in url)\n",
    "    features['DigitRatioInURL'] = features['NoOfDigitsInURL'] / features['URLLength'] if features['URLLength'] > 0 else 0\n",
    "    features['NoOfEqualsInURL'] = url.count('=')\n",
    "    features['NoOfQMarkInURL'] = url.count('?')\n",
    "    features['NoOfAmpersandInURL'] = url.count('&')\n",
    "    features['NoOfOtherSpecialCharsInURL'] = len(re.findall(r'[^a-zA-Z0-9:/?&=]', url))\n",
    "    features['SpecialCharRatioInURL'] = features['NoOfOtherSpecialCharsInURL'] / features['URLLength'] if features['URLLength'] > 0 else 0\n",
    "    \n",
    "    # Protocol Check\n",
    "    features['IsHTTPS'] = 1 if url.startswith('https://') else 0\n",
    "    \n",
    "    # Fetch webpage content\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Approximate line of code\n",
    "        features['LineOfCode'] = len(soup.prettify().splitlines())\n",
    "        \n",
    "        # Largest line length\n",
    "        features['LargestLineLength'] = max(len(line) for line in soup.prettify().splitlines())\n",
    "        \n",
    "        # Title check\n",
    "        title_tag = soup.find('title')\n",
    "        features['HasTitle'] = 1 if title_tag else 0\n",
    "        features['Title'] = title_tag.text if title_tag else ''\n",
    "        \n",
    "        # Domain Title Match Score\n",
    "        features['DomainTitleMatchScore'] = calculate_domain_title_match_score(features['Title'], features['Domain'])\n",
    "        \n",
    "        # URL Title Match Score\n",
    "        url_path = urlparse(url).path\n",
    "        features['URLTitleMatchScore'] = calculate_url_title_match_score(features['Title'], url_path)\n",
    "        \n",
    "        # Other webpage attributes\n",
    "        features['HasFavicon'] = 1 if soup.find('link', rel='icon') else 0\n",
    "        features['Robots'] = 1 if requests.get(url + '/robots.txt').status_code == 200 else 0\n",
    "        features['IsResponsive'] = 1 if soup.find('meta', attrs={'name': 'viewport'}) else 0\n",
    "        features['NoOfURLRedirect'] = len(response.history)\n",
    "        \n",
    "        # Meta tags and features\n",
    "        features['HasDescription'] = 1 if soup.find('meta', attrs={'name': 'description'}) else 0\n",
    "        features['NoOfPopup'] = count_popups(soup)\n",
    "        features['NoOfiFrame'] = len(soup.find_all('iframe'))\n",
    "        features['HasExternalFormSubmit'] = 1 if soup.find('form', action=lambda x: x and not x.startswith('/')) else 0\n",
    "        features['HasSocialNet'] = 1 if soup.find('a', href=lambda x: 'facebook.com' in x or 'twitter.com' in x) else 0\n",
    "        features['HasSubmitButton'] = 1 if soup.find('input', type='submit') else 0\n",
    "        features['HasHiddenFields'] = 1 if soup.find('input', type='hidden') else 0\n",
    "        features['HasPasswordField'] = 1 if soup.find('input', type='password') else 0\n",
    "        features['Bank'] = 1 if 'bank' in url.lower() else 0\n",
    "        features['Pay'] = 1 if 'pay' in url.lower() else 0\n",
    "        features['Crypto'] = 1 if 'crypto' in url.lower() else 0\n",
    "        features['HasCopyrightInfo'] = 1 if soup.find(text=re.compile(r'copyright', re.I)) else 0\n",
    "        features['NoOfImage'] = len(soup.find_all('img'))\n",
    "        features['NoOfCSS'] = len(soup.find_all('link', rel='stylesheet'))\n",
    "        features['NoOfJS'] = len(soup.find_all('script'))\n",
    "        features['NoOfSelfRef'] = len(soup.find_all('a', href=lambda x: x and x.startswith(url)))\n",
    "        features['NoOfEmptyRef'] = len(soup.find_all('a', href=''))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame([features])\n",
    "\n",
    "def calculate_continuation_rate(url):\n",
    "    continuation_count = sum(url[i] == url[i + 1] for i in range(len(url) - 1))\n",
    "    return continuation_count / (len(url) - 1) if len(url) > 1 else 0\n",
    "\n",
    "def calculate_similarity_index(url, keywords=[\"bank\", \"login\", \"secure\", \"account\"]):\n",
    "    vectorizer = TfidfVectorizer().fit_transform([url] + keywords)\n",
    "    vectors = vectorizer.toarray()\n",
    "    similarity_matrix = cosine_similarity(vectors)\n",
    "    return max(similarity_matrix[0][1:])\n",
    "\n",
    "def calculate_domain_title_match_score(title, domain):\n",
    "    if not title or not domain:\n",
    "        return 0\n",
    "    vectorizer = TfidfVectorizer().fit_transform([title, domain])\n",
    "    vectors = vectorizer.toarray()\n",
    "    similarity_matrix = cosine_similarity(vectors)\n",
    "    return similarity_matrix[0][1]\n",
    "\n",
    "def calculate_url_title_match_score(title, url_path):\n",
    "    if not title or not url_path:\n",
    "        return 0\n",
    "    vectorizer = TfidfVectorizer().fit_transform([title, url_path])\n",
    "    vectors = vectorizer.toarray()\n",
    "    similarity_matrix = cosine_similarity(vectors)\n",
    "    return similarity_matrix[0][1]\n",
    "\n",
    "def count_popups(soup):\n",
    "    popup_classes = ['modal', 'popup', 'overlay']\n",
    "    popups = [soup.find_all(class_=cls) for cls in popup_classes]\n",
    "    return sum(len(p) for p in popups)\n",
    "\n",
    "def clean_url(url):\n",
    "    return url.strip()\n",
    "\n",
    "# Example usage\n",
    "url1 = input(\"Enter a URL to analyze: \")\n",
    "url = clean_url(url1)\n",
    "features_df = analyze_url(url)\n",
    "print(features_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cl501_27\\AppData\\Local\\Temp\\ipykernel_11440\\3325994772.py:98: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  features['HasCopyrightInfo'] = 1 if soup.find(text=re.compile(r'copyright', re.I)) else 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DomainLength</th>\n",
       "      <th>IsDomainIP</th>\n",
       "      <th>URLSimilarityIndex</th>\n",
       "      <th>CharContinuationRate</th>\n",
       "      <th>TLDLegitimateProb</th>\n",
       "      <th>URLCharProb</th>\n",
       "      <th>TLDLength</th>\n",
       "      <th>NoOfSubDomain</th>\n",
       "      <th>HasObfuscation</th>\n",
       "      <th>NoOfObfuscatedChar</th>\n",
       "      <th>...</th>\n",
       "      <th>Bank</th>\n",
       "      <th>Pay</th>\n",
       "      <th>Crypto</th>\n",
       "      <th>HasCopyrightInfo</th>\n",
       "      <th>NoOfImage</th>\n",
       "      <th>NoOfCSS</th>\n",
       "      <th>NoOfJS</th>\n",
       "      <th>NoOfSelfRef</th>\n",
       "      <th>NoOfEmptyRef</th>\n",
       "      <th>NoOfExternalRef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   DomainLength  IsDomainIP  URLSimilarityIndex  CharContinuationRate  \\\n",
       "0            12           0                 0.0              0.148148   \n",
       "\n",
       "   TLDLegitimateProb  URLCharProb  TLDLength  NoOfSubDomain  HasObfuscation  \\\n",
       "0                0.9     0.058824          2              1               0   \n",
       "\n",
       "   NoOfObfuscatedChar  ...  Bank  Pay  Crypto  HasCopyrightInfo  NoOfImage  \\\n",
       "0                   2  ...     1    0       0                 0          0   \n",
       "\n",
       "   NoOfCSS  NoOfJS  NoOfSelfRef  NoOfEmptyRef  NoOfExternalRef  \n",
       "0        1       0            0             0                0  \n",
       "\n",
       "[1 rows x 49 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Initialize tldextract for domain extraction\n",
    "extractor = tldextract.TLDExtract(cache_dir=False)\n",
    "\n",
    "def analyze_url(url):\n",
    "    features = {}\n",
    "    \n",
    "    # Parse URL components\n",
    "    extracted = extractor(url)\n",
    "    protocol = urlparse(url).scheme  # Get the protocol (http or https)\n",
    "    full_domain = protocol + ('.' + extracted.subdomain if extracted.subdomain else '') + '.' + extracted.domain\n",
    "    \n",
    "    # 1. Domain Length (excluding protocol)\n",
    "    features['DomainLength'] = len(extracted.domain)\n",
    "    \n",
    "    # 2. Is Domain an IP Address?\n",
    "    features['IsDomainIP'] = 1 if re.match(r'^\\d{1,3}(\\.\\d{1,3}){3}$', extracted.domain) else 0\n",
    "    \n",
    "    # 3. URL Similarity Index (Cosine similarity with risky keywords)\n",
    "    features['URLSimilarityIndex'] = calculate_similarity_index(url)\n",
    "    \n",
    "    # 4. Char Continuation Rate (rate of repeated characters in URL)\n",
    "    features['CharContinuationRate'] = calculate_continuation_rate(url)\n",
    "    \n",
    "    # 5. TLD Legitimate Probability (this could be complex, so it's a placeholder)\n",
    "    features['TLDLegitimateProb'] = 0.9  # Placeholder example value\n",
    "    \n",
    "    # 6. URL Character Probability (basic probability check using character frequencies)\n",
    "    features['URLCharProb'] = calculate_char_probability(url)\n",
    "    \n",
    "    # 7. TLD Length (length of the TLD part)\n",
    "    features['TLDLength'] = len(extracted.suffix)\n",
    "    \n",
    "    # 8. Number of Subdomains\n",
    "    features['NoOfSubDomain'] = len(extracted.subdomain.split('.')) if extracted.subdomain else 0\n",
    "    \n",
    "    # 9. Obfuscation: Check if URL has obfuscation patterns (%20, _, -, etc.)\n",
    "    features['HasObfuscation'] = 1 if has_obfuscation(url) else 0\n",
    "    features['NoOfObfuscatedChar'] = len(re.findall(r'%|\\.|_|-|@|%20', url))\n",
    "    features['ObfuscationRatio'] = features['NoOfObfuscatedChar'] / len(url) if len(url) > 0 else 0\n",
    "    \n",
    "    # 10. Letters in URL (only letters)\n",
    "    features['NoOfLettersInURL'] = sum(c.isalpha() for c in url)\n",
    "    features['LetterRatioInURL'] = features['NoOfLettersInURL'] / len(url) if len(url) > 0 else 0\n",
    "    \n",
    "    # 11. Digits in URL (only digits)\n",
    "    features['NoOfDegitsInURL'] = sum(c.isdigit() for c in url)\n",
    "    features['DegitRatioInURL'] = features['NoOfDegitsInURL'] / len(url) if len(url) > 0 else 0\n",
    "    \n",
    "    # 12. Count specific symbols in URL\n",
    "    features['NoOfEqualsInURL'] = url.count('=')\n",
    "    features['NoOfQMarkInURL'] = url.count('?')\n",
    "    features['NoOfAmpersandInURL'] = url.count('&')\n",
    "    features['NoOfOtherSpecialCharsInURL'] = len(re.findall(r'[^a-zA-Z0-9:/?&=]', url))\n",
    "    features['SpacialCharRatioInURL'] = features['NoOfOtherSpecialCharsInURL'] / len(url) if len(url) > 0 else 0\n",
    "    \n",
    "    # 13. HTTPS check\n",
    "    features['IsHTTPS'] = 1 if url.startswith('https://') else 0\n",
    "    \n",
    "    # Webpage-specific features (scraping)\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        features['LineOfCode'] = len(soup.prettify().splitlines())\n",
    "        features['LargestLineLength'] = max(len(line) for line in soup.prettify().splitlines())\n",
    "        \n",
    "        title_tag = soup.find('title')\n",
    "        features['HasTitle'] = 1 if title_tag else 0\n",
    "        features['DomainTitleMatchScore'] = calculate_domain_title_match_score(title_tag.text if title_tag else '', extracted.domain)\n",
    "        url_path = urlparse(url).path\n",
    "        features['URLTitleMatchScore'] = calculate_url_title_match_score(title_tag.text if title_tag else '', url_path)\n",
    "        \n",
    "        features['HasFavicon'] = 1 if soup.find('link', rel='icon') else 0\n",
    "        features['Robots'] = 1 if requests.get(url + '/robots.txt').status_code == 200 else 0\n",
    "        features['IsResponsive'] = 1 if soup.find('meta', attrs={'name': 'viewport'}) else 0\n",
    "        features['NoOfURLRedirect'] = len(response.history)\n",
    "        features['NoOfSelfRedirect'] = len(soup.find_all('a', href=lambda x: x and x.startswith(url)))\n",
    "        features['HasDescription'] = 1 if soup.find('meta', attrs={'name': 'description'}) else 0\n",
    "        features['NoOfPopup'] = count_popups(soup)\n",
    "        features['NoOfiFrame'] = len(soup.find_all('iframe'))\n",
    "        features['HasExternalFormSubmit'] = 1 if soup.find('form', action=lambda x: x and not x.startswith('/')) else 0\n",
    "        features['HasSocialNet'] = 1 if soup.find('a', href=lambda x: 'facebook.com' in x or 'twitter.com' in x) else 0\n",
    "        features['HasSubmitButton'] = 1 if soup.find('input', type='submit') else 0\n",
    "        features['HasHiddenFields'] = 1 if soup.find('input', type='hidden') else 0\n",
    "        features['HasPasswordField'] = 1 if soup.find('input', type='password') else 0\n",
    "        features['Bank'] = 1 if 'bank' in url.lower() else 0\n",
    "        features['Pay'] = 1 if 'pay' in url.lower() else 0\n",
    "        features['Crypto'] = 1 if 'crypto' in url.lower() else 0\n",
    "        features['HasCopyrightInfo'] = 1 if soup.find(text=re.compile(r'copyright', re.I)) else 0\n",
    "        features['NoOfImage'] = len(soup.find_all('img'))\n",
    "        features['NoOfCSS'] = len(soup.find_all('link', rel='stylesheet'))\n",
    "        features['NoOfJS'] = len(soup.find_all('script'))\n",
    "        features['NoOfSelfRef'] = len(soup.find_all('a', href=lambda x: x and x.startswith(url)))\n",
    "        features['NoOfEmptyRef'] = len(soup.find_all('a', href=''))\n",
    "        features['NoOfExternalRef'] = len(soup.find_all('a', href=lambda x: x and not x.startswith(url) and not x.startswith('/')))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame([features])\n",
    "\n",
    "# Helper functions\n",
    "def calculate_continuation_rate(url):\n",
    "    continuation_count = sum(url[i] == url[i + 1] for i in range(len(url) - 1))\n",
    "    return continuation_count / (len(url) - 1) if len(url) > 1 else 0\n",
    "\n",
    "def calculate_similarity_index(url, keywords=[\"bank\", \"login\", \"secure\", \"account\"]):\n",
    "    vectorizer = TfidfVectorizer().fit_transform([url] + keywords)\n",
    "    vectors = vectorizer.toarray()\n",
    "    similarity_matrix = cosine_similarity(vectors)\n",
    "    return max(similarity_matrix[0][1:])\n",
    "\n",
    "def calculate_char_probability(url):\n",
    "    char_counts = {char: url.count(char) for char in set(url)}\n",
    "    total_chars = len(url)\n",
    "    probabilities = {char: count / total_chars for char, count in char_counts.items()}\n",
    "    return sum(probabilities.values()) / len(probabilities) if probabilities else 0\n",
    "\n",
    "def has_obfuscation(url):\n",
    "    return bool(re.search(r'%[0-9A-F]{2}|_|-|@|%20', url))\n",
    "\n",
    "def calculate_domain_title_match_score(title, domain):\n",
    "    title_words = set(title.lower().split())\n",
    "    domain_words = set(domain.lower().split('.'))\n",
    "    return len(title_words & domain_words) / len(title_words) if title_words else 0\n",
    "\n",
    "def calculate_url_title_match_score(title, path):\n",
    "    path_words = set(path.lower().split('/'))\n",
    "    title_words = set(title.lower().split())\n",
    "    return len(path_words & title_words) / len(path_words) if path_words else 0\n",
    "\n",
    "def count_popups(soup):\n",
    "    # Custom function to count possible popups\n",
    "    return len(soup.find_all('script', src=lambda x: x and 'popup' in x))\n",
    "\n",
    "# Test with URL\n",
    "url1 = input(\"Enter a URL to analyze: \")\n",
    "\n",
    "features_df = analyze_url(url1)\n",
    "features_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DomainLength', 'IsDomainIP', 'URLSimilarityIndex',\n",
       "       'CharContinuationRate', 'TLDLegitimateProb', 'URLCharProb', 'TLDLength',\n",
       "       'NoOfSubDomain', 'HasObfuscation', 'NoOfObfuscatedChar',\n",
       "       'ObfuscationRatio', 'NoOfLettersInURL', 'LetterRatioInURL',\n",
       "       'NoOfDegitsInURL', 'DegitRatioInURL', 'NoOfEqualsInURL',\n",
       "       'NoOfQMarkInURL', 'NoOfAmpersandInURL', 'NoOfOtherSpecialCharsInURL',\n",
       "       'SpacialCharRatioInURL', 'IsHTTPS', 'LineOfCode', 'LargestLineLength',\n",
       "       'HasTitle', 'DomainTitleMatchScore', 'URLTitleMatchScore', 'HasFavicon',\n",
       "       'Robots', 'IsResponsive', 'NoOfURLRedirect', 'NoOfSelfRedirect',\n",
       "       'HasDescription', 'NoOfPopup', 'NoOfiFrame', 'HasExternalFormSubmit',\n",
       "       'HasSocialNet', 'HasSubmitButton', 'HasHiddenFields',\n",
       "       'HasPasswordField', 'Bank', 'Pay', 'Crypto', 'HasCopyrightInfo',\n",
       "       'NoOfImage', 'NoOfCSS', 'NoOfJS', 'NoOfSelfRef', 'NoOfEmptyRef',\n",
       "       'NoOfExternalRef'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=\"hdfcbank\"\n",
    "'bank' in a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
